import streamlit as st
import re
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer
from huggingface_hub import InferenceClient

# ì¶”ì²œì‹œìŠ¤í…œ

# ì„¤ì •

VISION_MODEL_ID = "openbmb/MiniCPM-Llama3-V-2_5" 
TEXT_MODEL_ID = "google/gemma-2-9b-it"


# í† í° ë¡œë”©

def get_huggingface_tokens():
    return {
        "gemma": st.secrets.get("HUGGINGFACE_API_TOKEN_GEMMA")
    }

def get_huggingface_token(model_type):
    return get_huggingface_tokens().get(model_type)


# ì…ë ¥ ì •ì œ

def clean_input(text: str) -> str:
    return re.sub(r"(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ë§í•´ ì¤˜)", "", text).strip()


# í…ìŠ¤íŠ¸ ìƒì„± (Gemma API)

def generate_text_via_api(prompt: str, model_name: str = TEXT_MODEL_ID) -> str:
    token = get_huggingface_token("gemma")
    if not token:
        st.error("âŒ Hugging Face API í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return ""

    # ì‚¬ìš©ì ì§ˆë¬¸

    system_prompt = """[ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­]
        ### 1. ë¶„ì„ ìš”êµ¬ì‚¬í•­
        **ğŸ–¼ï¸ ì´ë¯¸ì§€ ë¶„ì„**
        - ì°¨ëŸ‰ ëª¨ë¸ íŠ¹ì„± ì‹ë³„:
        - ì—°ì‹: Â±1ë…„ ì˜¤ì°¨ í—ˆìš© (ì˜ˆ: 2024-2026)
        - ë””ìì¸ ìš”ì†Œ: ì „ë©´ ê·¸ë¦´/í—¤ë“œë¼ì´íŠ¸/íœ  ë””ìì¸ ìƒì„¸ ë¶„ì„
        - ê¸°ìˆ  ì‚¬ì–‘: ë°°í„°ë¦¬ ìš©ëŸ‰ (Â±5kWh), ì£¼í–‰ê±°ë¦¬ (NEDC Â±30km)

        **ğŸ“ í…ìŠ¤íŠ¸ ë¶„ì„**
        - í˜„ëŒ€ì°¨ê·¸ë£¹ 2025 ëª©í‘œ: 739ë§Œ ëŒ€ ì¤‘ ì „ê¸°ì°¨ 67ë§Œ ëŒ€
        - ê¸€ë¡œë²Œ ì „ê¸°ì°¨ ì‹œì¥: 2025ë…„ 1,160ë§Œ ëŒ€ (ì „ë…„æ¯” +30%)
        - ë¦¬ìŠ¤í¬: BYD ê°€ê²©ê²½ìŸ, Euro7 ê·œì œ, ì¶©ì „ í‘œì¤€í™” ì§€ì—°

        ### 2. ì¶œë ¥ í˜•ì‹
        ## ğŸ“Š 2025 í˜„ëŒ€/ê¸°ì•„ ì „ê¸°ì°¨ ì „ëµ ë³´ê³ ì„œ
        | êµ¬ë¶„ | 2024 | 2025ì˜ˆìƒ | ë¹„ì¤‘ |
        |------|------|----------|------|
        | ê¸€ë¡œë²Œ ì‹œì¥ | 890ë§Œ | 1,160ë§Œ | +30% |
        | í˜„ëŒ€/ê¸°ì•„ | 48ë§Œ | 67ë§Œ | 7.2% |

        - ë¦¬ìŠ¤í¬ ìš”ì¸: ê°€ê²©ê²½ìŸ, ê·œì œë¹„ìš© ì¦ê°€
        - ì „ëµ: ë¶ë¯¸ ì•„ì´ì˜¤ë‹‰9, ìœ ëŸ½ EV5 í™•ëŒ€, ì¤‘êµ­ X-GMP íˆ¬ì
        """

    text_prompt = [
        "í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì„œ ì‘ì„±í•´ì¤˜",
        "í˜„ëŒ€&ê¸°ì•„ ë¶„ì„ì„ ì¤‘ì ìœ¼ë¡œ ì‘ì„±í•´ì¤˜",
        "AI í•™ìŠµ ëª¨ë¸ì˜ ë¶„ì„ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±í•´ì¤˜",
        "ìµœê·¼ 10ë…„ê°„ì˜ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•´ì¤˜",
        "ê¸€ë¡œë²Œ ì‹œì¥ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì‘ì„±í•´ì¤˜",
        "ìµœê·¼ 5ë…„ê°„ì˜ íŠ¸ë Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•´ì¤˜",
        "í˜„ì¬ ë‰´ìŠ¤ë¥¼ ì°¸ê³ í•´ì„œ ë¶„ì„í•´ì¤˜",
        "ì‹œì¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ í¬í•¨í•´ì¤˜",
        "í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜",
        "ê¸ì •ì /ë¶€ì •ì  ìš”ì¸ì„ ë‚˜ëˆ ì„œ ì •ë¦¬í•´ì¤˜",
        "3ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤(ë‚™ê´€/ì¤‘ë¦½/ë¹„ê´€)ë¡œ ì˜ˆì¸¡í•´ì¤˜",
        "ê°„ê²°í•˜ê²Œ í•µì‹¬ ìœ„ì£¼ë¡œ ìš”ì•½í•´ì¤˜",
        "ì „ë¬¸ê°€ì²˜ëŸ¼ ê°ê´€ì ì¸ í†¤ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜",
        "ìµœê·¼ 1ë…„ê°„ ë³€í™”ëœ íë¦„ì„ ë°˜ì˜í•´ì„œ ì„¤ëª…í•´ì¤˜",
        "ì‹œì¥íë¦„ì„ ìš°ì„ ì ìœ¼ë¡œ ì–¸ê¸‰í•˜ê³  ë‹¤ìŒì— ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸ì„ ì–¸ê¸‰í•´ì¤˜",
        "ì˜ˆìƒ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•´ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜"
    ]

    full_prompt = f"{system_prompt}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{prompt}\n\n[ì¶”ê°€ ì§€ì‹œì‚¬í•­]\n" + "\n".join(text_prompt)


    try:
        client = InferenceClient(model=model_name, token=token)
        response = client.text_generation(
            prompt=f"ë‹¤ìŒ ìš”ì²­ì— ë§ëŠ” ë¶„ì„ì„ ì „ë¬¸ê°€ì²˜ëŸ¼ 500ì ë‚´ì™¸ë¡œ ìš”ì•½í•´ì¤˜:\n{full_prompt}",
            max_new_tokens=700,
            temperature=0.7
        )
        return response
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return ""


# ì´ë¯¸ì§€ ë¶„ì„ (MiniCPM-Llama3-V ëª¨ë¸ ë¡œì»¬ ì‹¤í–‰)

@st.cache_resource
def load_minicpm_model():
    tokenizer = AutoTokenizer.from_pretrained(VISION_MODEL_ID, trust_remote_code=True)
    model = AutoModel.from_pretrained(
        VISION_MODEL_ID,
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
    ).to("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    return model, tokenizer

def analyze_image_with_minicpm(image: Image.Image, question: str) -> str:
    try:
        model, tokenizer = load_minicpm_model()
        msgs = [{"role": "user", "content": question}]
        result = model.chat(image=image, msgs=msgs, tokenizer=tokenizer, sampling=True, temperature=0.7)
        return result
    except Exception as e:
        return f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì˜¤ë¥˜: {e}"


# Streamlit UI

def recommendations_ui():

    st.title("ğŸ¤– AI ìˆ˜ì¶œ ë¶„ì„ & ì°¨ëŸ‰ ì¶”ì²œ ì‹œìŠ¤í…œ")
    st.caption("ì´ë¯¸ì§€ ì—…ë¡œë“œì™€ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ì°¨ëŸ‰ íŠ¹ì§• ë° ì‹œì¥ ì˜ˆì¸¡ì„ ë¶„ì„í•©ë‹ˆë‹¤.")

    with st.form("analyze_form"):
        image_file = st.file_uploader("ğŸ“· ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì°¨ëŸ‰, ë¬¸ì„œ ë“±)", type=["jpg", "jpeg", "png"])
        user_input = st.text_area("ğŸ“ ë¶„ì„í•  ì§ˆë¬¸ ì…ë ¥", placeholder="ì˜ˆ: ì´ ì°¨ëŸ‰ì˜ ë¶ë¯¸ ìˆ˜ìš” ì˜ˆì¸¡ì€?")
        submitted = st.form_submit_button("ğŸš€ ë¶„ì„ ì‹¤í–‰")

    if submitted:
        st.markdown("---")
        results = []

        # ì´ë¯¸ì§€ ë¶„ì„
        if image_file:
            try:
                image = Image.open(image_file).convert("RGB")
                st.image(image, caption="ì—…ë¡œë“œëœ ì´ë¯¸ì§€", use_column_width=True)
                with st.spinner("ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ì¤‘..."):
                    result_img = analyze_image_with_minicpm(image, user_input or "ì´ ì°¨ëŸ‰ì˜ íŠ¹ì§•ì„ ë¶„ì„í•´ì¤˜")
                    results.append(f"### ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼\n{result_img}")
            except Exception as e:
                st.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        # í…ìŠ¤íŠ¸ ë¶„ì„
        if user_input:
            with st.spinner("ğŸ“Š í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‹œì¥ ì˜ˆì¸¡ ì¤‘..."):
                cleaned = clean_input(user_input)
                result_txt = generate_text_via_api(cleaned)
                results.append(f"### ğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼\n{result_txt}")

        # ê²°ê³¼ ì¶œë ¥
        if results:
            for res in results:
                st.markdown(res)
        else:
            st.warning("ì…ë ¥ ë‚´ìš©ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")


