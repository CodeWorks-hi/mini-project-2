import streamlit as st
import re
import requests
import torch
from PIL import Image
from io import BytesIO
import matplotlib.pyplot as plt
from transformers import AutoProcessor, LlavaForConditionalGeneration,TextStreamer
from transformers import AutoModel,AutoProcessor
from huggingface_hub import InferenceClient

# =====================
# ì„¤ì •
# =====================
VISION_MODEL_ID = "visheratin/MC-LLaVA-3b"
TEXT_MODEL_ID = "google/gemma-2-9b-it"
MAX_IMAGE_SIZE = 2048

# =====================
# í† í° ë¡œë”©
# =====================
def get_huggingface_tokens():
    return {
        "llava": st.secrets.get("HUGGINGFACE_API_TOKEN_LLAVA"),
        "gemma": st.secrets.get("HUGGINGFACE_API_TOKEN_GEMMA")
    }

def get_huggingface_token(model_type):
    return get_huggingface_tokens().get(model_type)

# =====================
# ì…ë ¥ ì •ì œ
# =====================
def clean_input(text: str) -> str:
    return re.sub(r"\b(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ë§í•´ ì¤˜)\b", "", text, flags=re.IGNORECASE).strip()

# =====================
# í…ìŠ¤íŠ¸ ìƒì„± (Gemma API)
# =====================
def generate_text_via_api(prompt: str, model_name: str = TEXT_MODEL_ID) -> str:
    token = get_huggingface_token("gemma")
    if not token:
        st.error("âŒ Hugging Face API í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return ""

    prompt_additions = [
        "í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì„œ ì‘ì„±í•´ì¤˜",
        "í˜„ì¬ ë‰´ìŠ¤ë¥¼ ì°¸ê³ í•´ì„œ ë¶„ì„í•´ì¤˜",
        "ì‹œì¥ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì¸ ê·¼ê±°ë¥¼ í¬í•¨í•´ì¤˜",
        "í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜",
        "ê¸ì •ì /ë¶€ì •ì  ìš”ì¸ì„ ë‚˜ëˆ ì„œ ì •ë¦¬í•´ì¤˜",
        "3ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤(ë‚™ê´€/ì¤‘ë¦½/ë¹„ê´€)ë¡œ ì˜ˆì¸¡í•´ì¤˜",
        "ê°„ê²°í•˜ê²Œ í•µì‹¬ ìœ„ì£¼ë¡œ ìš”ì•½í•´ì¤˜ (500ì ì´ë‚´)",
        "ì „ë¬¸ê°€ì²˜ëŸ¼ ê°ê´€ì ì¸ í†¤ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜",
        "ì£¼ìš” ë¦¬ìŠ¤í¬ ìš”ì¸ì„ ìš°ì„ ì ìœ¼ë¡œ ì–¸ê¸‰í•´ì¤˜",
        "ìµœê·¼ 1ë…„ê°„ ë³€í™”ëœ íë¦„ì„ ë°˜ì˜í•´ì„œ ì„¤ëª…í•´ì¤˜",
        "ì˜ˆìƒ ìˆ˜ì¹˜ë¥¼ í¬í•¨í•´ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜"
    ]

    enhanced_prompt = f"{prompt}\n\nì¶”ê°€ ì§€ì‹œì‚¬í•­:\n" + "\n".join(prompt_additions)

    try:
        client = InferenceClient(model=model_name, token=token)
        response = client.text_generation(
            prompt=f"ë‹¤ìŒ ìš”ì²­ì— ë§ëŠ” ë¶„ì„ ë° ì˜ˆì¸¡ ì •ë³´ë¥¼ ì „ë¬¸ê°€ì˜ ì‹œê°ìœ¼ë¡œ ì‘ì„±í•´ì¤˜:\n{enhanced_prompt}",
            max_new_tokens=512,
            temperature=0.7
        )
        return response
    except Exception as e:
        st.error(f"API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return ""

# =====================
# ì´ë¯¸ì§€ ë¶„ì„ (LLaVA ë¡œì»¬ ì‹¤í–‰)
# =====================
def analyze_image_with_llava(image: Image.Image, question: str) -> str:
    processor = AutoProcessor.from_pretrained(VISION_MODEL_ID, trust_remote_code=True)
    model = LlavaForConditionalGeneration.from_pretrained(
        VISION_MODEL_ID,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    )

    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": question}
        ],
    }]

    inputs = processor(
        text=processor.apply_chat_template(messages, add_generation_prompt=True),
        images=image,
        return_tensors="pt"
    ).to(model.device, torch.bfloat16)

    output = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.3,
        top_p=0.95,
        do_sample=True,
        repetition_penalty=1.2,
        use_cache=True
    )

    result = processor.decode(
        output[0],
        skip_special_tokens=True,
        clean_up_tokenization_spaces=True
    )
    return result

# =====================
# Streamlit UI
# =====================
def recommendations_ui():
    st.title("ğŸ¤– AI ìˆ˜ì¶œ ë¶„ì„ & ì°¨ëŸ‰ ì¶”ì²œ ì‹œìŠ¤í…œ")
    st.info("ì°¨ëŸ‰ ì´ë¯¸ì§€ë‚˜ ì§ˆë¬¸ì„ ì—…ë¡œë“œí•˜ë©´ ë¶„ì„ê³¼ ì˜ˆì¸¡ì„ ì œê³µí•©ë‹ˆë‹¤.")

    with st.form("recommend_form"):
        image_file = st.file_uploader(
            label="ğŸ“· ì°¨ëŸ‰ ë˜ëŠ” ë¬¸ì„œ ì´ë¯¸ì§€ ì—…ë¡œë“œ (ì„ íƒ)",
            type=["jpg", "png"],
            label_visibility="visible"
        )
        user_input = st.text_area(
            label="ğŸ“ ë¶„ì„ ì§ˆë¬¸ ì…ë ¥",
            placeholder="ì˜ˆ: ë¶ë¯¸ ì‹œì¥ì—ì„œ ì´ ì „ê¸° SUVì˜ ìˆ˜ìš” ì˜ˆì¸¡",
            height=120,
            label_visibility="visible"
        )
        submitted = st.form_submit_button("ğŸš€ ë¶„ì„ ì‹¤í–‰")

    if submitted:
        results = []

        # ì´ë¯¸ì§€ ì²˜ë¦¬
        if image_file:
            try:
                image = Image.open(image_file).convert("RGB")
                st.image(image, caption="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€", use_container_width=True)
                with st.spinner("ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ì¤‘..."):
                    result_img = analyze_image_with_llava(image, user_input or "ì´ ì°¨ëŸ‰ì˜ íŠ¹ì§•ê³¼ ì‹œì¥ ë¶„ì„ì„ í•´ì¤˜")
                    results.append(f"### ğŸ” ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼\n{result_img}")
            except Exception as e:
                st.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

        # í…ìŠ¤íŠ¸ ë¶„ì„
        if user_input:
            with st.spinner("ğŸ“Š í…ìŠ¤íŠ¸ ê¸°ë°˜ ì˜ˆì¸¡ ìƒì„± ì¤‘..."):
                cleaned = clean_input(user_input)
                result_text = generate_text_via_api(cleaned)
                results.append(f"### ğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼\n{result_text}")

        # ê²°ê³¼ ì¶œë ¥
        if results:
            st.divider()
            for r in results:
                st.markdown(r)
        else:
            st.warning("ì…ë ¥ ë‚´ìš© ë˜ëŠ” ì´ë¯¸ì§€ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
