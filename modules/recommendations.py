import streamlit as st
import re
import requests
from huggingface_hub import InferenceClient
from bs4 import BeautifulSoup

TEXT_MODEL_ID = "google/gemma-2-9b-it"

def fetch_latest_news(keyword="í˜„ëŒ€ìë™ì°¨", count=5):
    try:
        search_url = f"https://search.naver.com/search.naver?where=news&query={keyword}"
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(search_url, headers=headers)
        soup = BeautifulSoup(response.text, 'html.parser')
        
        news_items = []
        news_elements = soup.select('.news_area')
        
        for i, item in enumerate(news_elements):
            if i >= count:
                break
                
            title_element = item.select_one('.news_tit')
            title = title_element.text if title_element else "ì œëª© ì—†ìŒ"
            link = title_element['href'] if title_element else ""
            
            summary_element = item.select_one('.dsc_wrap')
            summary = summary_element.text.strip() if summary_element else "ìš”ì•½ ì—†ìŒ"
            
            date_element = item.select_one('.info_group span.info')
            date = date_element.text if date_element else "ë‚ ì§œ ì—†ìŒ"
            
            news_items.append({
                'title': title,
                'link': link,
                'description': summary,
                'pubDate': date
            })
        
        st.session_state.latest_news = news_items
        return news_items
        
    except Exception as e:
        st.error(f"ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸° ì˜¤ë¥˜: {e}")
        return []

def get_huggingface_token(model_type):
    tokens = {"gemma": st.secrets.get("HUGGINGFACE_API_TOKEN_GEMMA")}
    return tokens.get(model_type)

def clean_input(text: str) -> str:
    return re.sub(r"(í•´ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…í•´ ì¤˜|ë§í•´ ì¤˜)", "", text).strip()

def clean_html_tags(text):
    return re.sub(r'<[^>]+>', '', text)

def format_news_for_prompt(news_items):
    if not news_items:
        return "ìµœì‹  ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    formatted_news = "## ìµœì‹  ìë™ì°¨ ì‚°ì—… ë‰´ìŠ¤\n\n"
    for i, item in enumerate(news_items[:5], 1):
        formatted_news += f"{i}. {item['title']} ({item['pubDate']})\n"
        formatted_news += f"   ìš”ì•½: {item['description'][:100]}...\n\n"
    
    return formatted_news

def format_predictions_for_api(predictions):
    if not predictions:
        return {}
        
    pred_type = predictions.get('type', '')
    
    if pred_type == 'region':
        region_name = predictions.get('name', 'ì§€ì—­ ì •ë³´ ì—†ìŒ')
        forecast = predictions.get('forecast', {})
        
        result = f"## {region_name} ì§€ì—­ ìˆ˜ì¶œëŸ‰ ì˜ˆì¸¡\n\n"
        result += "| ì—°ë„ | ì›” | ì˜ˆì¸¡ ìˆ˜ì¶œëŸ‰ |\n|-----|-----|-----|\n"
        
        years = forecast.get('ì—°ë„', {})
        months = forecast.get('ì›”', {})
        exports = forecast.get('ì˜ˆì¸¡ ìˆ˜ì¶œëŸ‰', {})
        
        if years and months and exports:
            for i in range(len(years)):
                result += f"| {years.get(str(i), '')} | {months.get(str(i), '')} | {exports.get(str(i), '')} |\n"
        
        return result
    
    elif pred_type == 'car':
        car_name = predictions.get('name', 'ì°¨ì¢… ì •ë³´ ì—†ìŒ')
        forecast = predictions.get('forecast', {})
        
        result = f"## {car_name} ì°¨ì¢… íŒë§¤ëŸ‰ ì˜ˆì¸¡\n\n"
        result += "| ì—°ë„ | ì›” | ì˜ˆì¸¡ íŒë§¤ëŸ‰ |\n|-----|-----|-----|\n"
        
        years = forecast.get('ì—°ë„', {})
        months = forecast.get('ì›”', {})
        sales = forecast.get('ì˜ˆì¸¡ íŒë§¤ëŸ‰', {})
        
        if years and months and sales:
            for i in range(len(years)):
                result += f"| {years.get(str(i), '')} | {months.get(str(i), '')} | {sales.get(str(i), '')} |\n"
        
        return result
    
    elif pred_type == 'plant':
        plant_name = predictions.get('name', 'ê³µì¥ ì •ë³´ ì—†ìŒ')
        forecast = predictions.get('forecast', {})
        
        result = f"## {plant_name} ê³µì¥ ìƒì‚°ëŸ‰ ì˜ˆì¸¡\n\n"
        result += "| ì—°ë„ | ì›” | ì˜ˆì¸¡ ìƒì‚°ëŸ‰ |\n|-----|-----|-----|\n"
        
        years = forecast.get('ì—°ë„', {})
        months = forecast.get('ì›”', {})
        production = forecast.get('ì˜ˆì¸¡ íŒë§¤ëŸ‰', {})
        
        if years and months and production:
            for i in range(len(years)):
                result += f"| {years.get(str(i), '')} | {months.get(str(i), '')} | {production.get(str(i), '')} |\n"
        
        return result
    
    return str(predictions)

def generate_text_via_api(prompt: str, predictions: dict, news_items: list, model_name: str = TEXT_MODEL_ID) -> str:
    token = get_huggingface_token("gemma")
    if not token:
        st.error("Hugging Face API í† í°ì´ ì—†ìŠµë‹ˆë‹¤.")
        return ""

    # ì˜ˆì¸¡ ë°ì´í„° í˜•ì‹ ë³€í™˜
    predictions_formatted = format_predictions_for_api(predictions)
    
    # ë‰´ìŠ¤ í…ìŠ¤íŠ¸ í¬ë§·íŒ…
    news_text = format_news_for_prompt(news_items)
    
    system_prompt = """
    [ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­]
    ###  ë¶„ì„ ëª©í‘œ
    - {news_text}ë¥¼ ì°¸ê³ í•˜ì—¬ ìµœì‹  ìë™ì°¨ ì‚°ì—… ë™í–¥ì„ ë°˜ì˜í•œ ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
    - {predictions_formatted}ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
    - {text_prompt}ì— ë”°ë¼ ìš”ì•½ëœ ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
    - ì•„ë˜ì˜ í˜•ì‹ì„ ë”°ë¼ í‘œì™€ ìš”ì•½ ë¶„ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

    ---

    ###  í‘œ í˜•ì‹ ì¶œë ¥ (ì˜ˆì‹œ)
    ## 2025 í˜„ëŒ€/ê¸°ì•„ ì „ê¸°ì°¨ ì „ëµ ìš”ì•½ ë³´ê³ ì„œ

    | í•­ëª©          | 2024 ìˆ˜ì¹˜ | 2025 ì˜ˆì¸¡ | ì¦ê°ë¥  ë˜ëŠ” ë¹„ì¤‘ |
    |---------------|------------|------------|------------------|
    | ê¸€ë¡œë²Œ EV ì‹œì¥ | 890ë§ŒëŒ€    | 1,160ë§ŒëŒ€  | +30%             |
    | í˜„ëŒ€/ê¸°ì•„ EV   | 48ë§ŒëŒ€     | 67ë§ŒëŒ€     | 7.2% ì ìœ ìœ¨      |

    ---

    ###  ì‹œì¥ ë™í–¥ ìš”ì•½
    - ê¸€ë¡œë²Œ EV ìˆ˜ìš”ëŠ” 30% ì¦ê°€ ì˜ˆìƒ
    - ë¶ë¯¸Â·ìœ ëŸ½ ì¤‘ì‹¬ìœ¼ë¡œ ê³ ì„±ì¥ì„¸ ìœ ì§€

    ---

    ### âš ï¸ ì£¼ìš” ë¦¬ìŠ¤í¬
    - ì¤‘êµ­ BYDì˜ ê°€ê²© ê²½ìŸ ì‹¬í™”
    - Euro7 ê·œì œ ëŒ€ì‘ ë¹„ìš© ì¦ê°€
    - ì¶©ì „ ì¸í”„ë¼ í‘œì¤€í™” ì§€ì—°

    ---

    ###  ì „ëµ ì œì•ˆ
    - ë¶ë¯¸: ì•„ì´ì˜¤ë‹‰9 ë“± SUV ë¼ì¸ì—… ê°•í™”
    - ìœ ëŸ½: EV5 ì¤‘ì‹¬ íŒë§¤ ì±„ë„ í™•ì¥
    - ì¤‘êµ­: X-GMP í”Œë«í¼ í˜„ì§€í™” íˆ¬ì ê°•í™”
    """

    text_prompt = [
        "í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì¤˜",
        "í˜„ëŒ€/ê¸°ì•„ ê¸€ë¡œë²Œ íŒë§¤ ì „ëµì„ ì¤‘ì ìœ¼ë¡œ ë¶„ì„í•´ì¤˜",
        "ìµœì‹  OECD ê²½ì œ ì „ë§ê³¼ í™˜ìœ¨ ë°ì´í„°ë¥¼ ë°˜ì˜í•´ì¤˜",
        "prediction.pyì˜ ì˜ˆì¸¡ ëª¨ë¸ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•´ì¤˜",
        "ê¸€ë¡œë²Œ ê²½ì œ ì„±ì¥ë¥  ë‘”í™”ì™€ ë¬´ì—­ ê¸´ì¥ ìƒí™©ì„ ê³ ë ¤í•´ì¤˜",
        "ì£¼ìš” ì‹œì¥ë³„(ë¶ë¯¸, ìœ ëŸ½, ì•„ì‹œì•„) íŒë§¤ ì „ëµì„ êµ¬ë¶„í•´ì„œ ì„¤ëª…í•´ì¤˜",
        "í™˜ìœ¨ ë³€ë™ì´ ìˆ˜ì¶œ ì „ëµì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•´ì¤˜",
        "ì¸í”Œë ˆì´ì…˜ê³¼ ì†Œë¹„ì êµ¬ë§¤ë ¥ ë³€í™”ë¥¼ ê³ ë ¤í•œ ì „ëµì„ ì œì‹œí•´ì¤˜",
        "í‘œ í˜•ì‹ìœ¼ë¡œ í•µì‹¬ ì§€í‘œë¥¼ ì •ë¦¬í•´ì¤˜",
        "ê²½ì œ ìƒí™©ì— ë”°ë¥¸ ê¸ì •ì /ë¶€ì •ì  ìš”ì¸ì„ êµ¬ë¶„í•´ì¤˜",
        "3ê°€ì§€ ì‹œë‚˜ë¦¬ì˜¤(ë‚™ê´€/ì¤‘ë¦½/ë¹„ê´€)ë¡œ íŒë§¤ëŸ‰ ì˜ˆì¸¡í•´ì¤˜",
        "ê°„ê²°í•˜ê²Œ í•µì‹¬ ìœ„ì£¼ë¡œ ìš”ì•½í•´ì¤˜",
        "ì „ë¬¸ê°€ì²˜ëŸ¼ ê°ê´€ì ì¸ í†¤ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜",
        "2025ë…„ 3ì›” ê¸°ì¤€ ìµœì‹  ê²½ì œ ë°ì´í„°ë¥¼ ë°˜ì˜í•´ì¤˜",
        "ê¸€ë¡œë²Œ ê²½ì œ íë¦„ì„ ë¨¼ì € ì„¤ëª…í•˜ê³  ìë™ì°¨ ì‚°ì—… ì˜í–¥ì„ ë¶„ì„í•´ì¤˜",
        "ì§€ì—­ë³„ íŒë§¤ ëª©í‘œì¹˜ì™€ í™˜ìœ¨ ì˜ˆì¸¡ì¹˜ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ í¬í•¨í•´ì¤˜",
        "ì°¨ëŸ‰ ì¶”ì²œ ì‹œìŠ¤í…œì˜ ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ í™œìš©í•œ ë§ì¶¤í˜• ì¶”ì²œ ë°©ì‹ì„ ì„¤ëª…í•´ì¤˜",
        "ê¸°ì¡´ ë°ì´í„° ë¶„ì„ì—ì„œ ê³ ê°ê³¼ íŒë§¤ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ í™œìš©í•˜ëŠ”ì§€ ìì„¸íˆ ì„¤ëª…í•´ì¤˜",
        "ë§ˆì¼€íŒ… ì „ëµ ìˆ˜ë¦½ ì‹œ ê³ ê° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ í”„ë¡œëª¨ì…˜ ì „ëµì˜ êµ¬ì²´ì ì¸ ì˜ˆì‹œë¥¼ ë“¤ì–´ì¤˜",
        "ë§¤ì¥ ì°¾ê¸° ì„œë¹„ìŠ¤ì˜ ìœ„ì¹˜ ê¸°ë°˜ ê¸°ëŠ¥ê³¼ ì‹¤ì‹œê°„ ì¬ê³  í™•ì¸ ê¸°ëŠ¥ì— ëŒ€í•´ ìƒì„¸íˆ ì„¤ëª…í•´ì¤˜",
        "ê´€ë¦¬ì ì„œë¹„ìŠ¤ì—ì„œ ì œê³µí•˜ëŠ” íŒë§¤ í˜„í™© ë° ì¬ê³  ê´€ë¦¬ ëŒ€ì‹œë³´ë“œì˜ ì£¼ìš” ê¸°ëŠ¥ë“¤ì„ ë‚˜ì—´í•´ì¤˜"
    ]

    # ì „ì²´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    full_prompt = f"{system_prompt}\n\n[ì˜ˆì¸¡ ë°ì´í„°]\n{predictions_formatted}\n\n[ìµœì‹  ë‰´ìŠ¤]\n{news_text}\n\n[ì‚¬ìš©ì ì§ˆë¬¸]\n{prompt}\n\n[ì¶”ê°€ ì§€ì‹œì‚¬í•­]\n" + "\n".join(text_prompt)
    
    try:
        client = InferenceClient(model=model_name, token=token)
        response = client.text_generation(
            prompt=f"ë‹¤ìŒ ìš”ì²­ì— ë§ëŠ” ë¶„ì„ì„ ì „ë¬¸ê°€ì²˜ëŸ¼ 1000ì ë‚´ì™¸ë¡œ ìš”ì•½í•´ì¤˜:\n{full_prompt}",
            max_new_tokens=1000,
            temperature=0.7
        )
        return response
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
        return ""

def recommendations_ui():
    st.title("AI ê¸°ë°˜ ì‹œì¥ ì˜ˆì¸¡ ë° ë¶„ì„")
    st.markdown("""
    - ì˜ˆì¸¡ ë°ì´í„°ì™€ ìµœì‹  ë‰´ìŠ¤ë¥¼ ì¢…í•©í•œ ì‹¬ì¸µ ë¶„ì„ ì œê³µ
    - ìµœì‹  AI ê¸°ìˆ ì„ í™œìš©í•œ ì‹œì¥ ë™í–¥ ì˜ˆì¸¡
    - ë°ì´í„° ê¸°ë°˜ì˜ ê°ê´€ì ì´ê³  í†µì°°ë ¥ ìˆëŠ” ê²°ê³¼ ë„ì¶œ
    """)

    if 'predictions' not in st.session_state:
        st.warning("ë¨¼ì € 'ì˜ˆì¸¡ ì‹œìŠ¤í…œ' íƒ­ì—ì„œ ì˜ˆì¸¡ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
        return

    with st.expander("ì˜ˆì¸¡ ê²°ê³¼ í‘œ (LSTM ê¸°ë°˜)"):
        st.write(st.session_state.predictions)
        formatted_predictions = format_predictions_for_api(st.session_state.predictions)
        st.markdown("### í˜•ì‹ ë³€í™˜ëœ ì˜ˆì¸¡ ë°ì´í„°")
        st.markdown(formatted_predictions)

    # ë‰´ìŠ¤ê°€ ì—†ìœ¼ë©´ ê°€ì ¸ì˜¤ê¸°
    if 'latest_news' not in st.session_state:
        with st.spinner("ìµœì‹  ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘..."):
            news_items = fetch_latest_news()
            if news_items:
                st.session_state.latest_news = news_items
            else:
                st.error("ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return

    # ìµœì‹  ë‰´ìŠ¤ í‘œì‹œ
    with st.expander("ğŸ“° ë¶„ì„ì— ì‚¬ìš©ëœ ìµœì‹  ë‰´ìŠ¤"):
        for i, news in enumerate(st.session_state.latest_news[:5], 1):
            title = clean_html_tags(news['title'])
            description = clean_html_tags(news['description'])
            
            st.markdown(
                f"""
                <div style="background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin-bottom: 15px; border-left: 4px solid #4285f4;">
                    <h4 style="margin-top: 0; color: #333;">{i}. {title}</h4>
                    <p style="color: #555;">{description[:150]}...</p>
                    <p style="color: #888; font-size: 0.8em;">{news.get('pubDate', '')}</p>
                </div>
                """, 
                unsafe_allow_html=True
            )

    # ì²« ë²ˆì§¸ í¼ (í‚¤: analyze_form)
    with st.form("analyze_form"):
        user_input = st.text_area("ì˜ˆì¸¡ ë° ë¶„ì„ ì…ë ¥", placeholder="ì˜ˆ: 2025ë…„ ë¯¸êµ­ ìˆ˜ì¶œ ì˜ˆì¸¡í•´ì¤˜")
        submitted = st.form_submit_button("ì˜ˆì¸¡ ë° ë¶„ì„ ì‹¤í–‰")
    
    if submitted:
        with st.spinner("ì‹œì¥ ì˜ˆì¸¡ ë° ë¶„ì„ ì¤‘..."):
            cleaned = clean_input(user_input)
            result_txt = generate_text_via_api(cleaned, st.session_state.predictions, st.session_state.latest_news)
            st.session_state.analysis_result = result_txt
            st.markdown(f"### ì¢…í•© ì˜ˆì¸¡ ë° ë¶„ì„ ê²°ê³¼\n{result_txt}")

    # ---- ì¤‘ë³µ í¼ ì œê±° ë˜ëŠ” í‚¤ ë³€ê²½ ----
    #
    # ì•„ë˜ì™€ ê°™ì€ ë‘ ë²ˆì§¸ í¼ì´ ì¤‘ë³µë˜ì–´ ìˆë‹¤ë©´, í‚¤ë¥¼ "analyze_form2"ì²˜ëŸ¼ ë°”ê¾¸ê±°ë‚˜
    # í¼ì´ ì‹¤ì œë¡œ í•„ìš” ì—†ë‹¤ë©´ ì œê±°í•´ì•¼ í•©ë‹ˆë‹¤.
    #
    # [ì˜ˆì‹œ] í‚¤ë¥¼ ë³€ê²½í•œ í¼
    #
    # with st.form("analyze_form2"):
    #     user_input2 = st.text_area("ì¶”ê°€ ë¶„ì„ ì…ë ¥", placeholder="ì˜ˆ: ì¤‘ë™ ì‹œì¥ ì§„ì¶œ ì „ëµ ì•Œë ¤ì¤˜")
    #     submitted2 = st.form_submit_button("ì¶”ê°€ ë¶„ì„ ì‹¤í–‰")
    # 
    # if submitted2:
    #     with st.spinner("ì¶”ê°€ ë¶„ì„ ì¤‘..."):
    #         cleaned2 = clean_input(user_input2)
    #         result_txt2 = generate_text_via_api(cleaned2, st.session_state.predictions, st.session_state.latest_news)
    #         st.markdown(f"### ì¶”ê°€ ë¶„ì„ ê²°ê³¼\n{result_txt2}")
